
# ##### We now calculate the defaulters and non-defaulters rate per decile.


# pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
# pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

# pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

# pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
# default_sum = pivot_df['Defaulter_Count'].sum()
# non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
# pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
# pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


# #####creating the cumulative sums
# pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


# #####adding Base value
# pivot_df['Base %'] = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] ######### enogh for gain chart

# #########LIFT CHART ######################
# pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# # final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
# pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
# pivot_df

# # -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # y_pred_2[0:1]
# # y_test_model.head(1)
# pred_proba_rfc = pd.DataFrame(predicted_proba_r)
# y_test_rfc = y_test_1.reset_index()
# y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
# y_test_rfc_label.head(1)


# # taking y_pred - predicted probabilities and y_hat -1,0
# y_pred_df=pd.DataFrame(pred_proba_rfc[1]) ### predicted probs
# y_hat_df=pd.DataFrame(y_pred_rfc_2)### 1,0

# #combining y_pred_df and y_hat_df and y_test
# predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
# predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
# predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
# predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],20,labels=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])
# predictions['LABEL_0']=1-predictions['LABEL']

# #creating Pivot table
# pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
#                      aggfunc={'LABEL':[np.sum],
#                               'LABEL_0':[np.sum],
#                               'PREDICTED_1_PROB' : [np.min,np.max]})

# ##### We now calculate the defaulters and non-defaulters rate per decile.


# pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
# pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

# pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

# pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
# default_sum = pivot_df['Defaulter_Count'].sum()
# non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
# pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
# pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


# #####creating the cumulative sums
# pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


# #####adding Base value
# pivot_df['Base %'] = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] ######### enogh for gain chart

# #########LIFT CHART ######################
# pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# # final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
# pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
# pivot_df

# # -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# # y_pred_2[0:1]
# # y_test_model.head(1)
# pred_proba_lgb = pd.DataFrame(predicted_proba_l)
# y_test_rfc = y_test_1.reset_index()
# y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
# y_test_rfc_label.head(1)


# # taking y_pred - predicted probabilities and y_hat -1,0
# y_pred_df=pd.DataFrame(pred_proba_lgb[1]) ### predicted probs
# y_hat_df=pd.DataFrame(y_pred_lgb_2)### 1,0

# #combining y_pred_df and y_hat_df and y_test
# predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
# predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
# predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
# predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],20,labels=['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20'])
# predictions['LABEL_0']=1-predictions['LABEL']

# #creating Pivot table
# pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
#                      aggfunc={'LABEL':[np.sum],
#                               'LABEL_0':[np.sum],
#                               'PREDICTED_1_PROB' : [np.min,np.max]})

# ##### We now calculate the defaulters and non-defaulters rate per decile.


# pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
# pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

# pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

# pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
# default_sum = pivot_df['Defaulter_Count'].sum()
# non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
# pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
# pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


# #####creating the cumulative sums
# pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


# #####adding Base value
# pivot_df['Base %'] = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100] ######### enogh for gain chart

# #########LIFT CHART ######################
# pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# # final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
# pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
# pivot_df

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# pred_proba_rfc = pd.DataFrame(predicted_proba_r)
# y_test_rfc = y_test_1.reset_index()
# y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
# y_test_rfc_label.head(1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
X_test_MSISDN["NR_SBSC"] = X_test_MSISDN["NR_SBSC"].astype(int)
X_test_msisdn = X_test_MSISDN.reset_index(drop=True)
combined_output_rfc_xgb_lgb = pd.concat([X_test_msisdn,final_columns_df_rfc, final_columns_df_xgb, final_columns_df_lgb], axis=1)
combined_output_rfc_xgb_lgb.head()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
combined_output_rfc_xgb_lgb.head()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
combined_output_rfc_xgb_lgb['COMBINED_LABEL'] = combined_output_rfc_xgb_lgb['LABEL_PRED_RFC'] + combined_output_rfc_xgb_lgb['LABEL_PRED_XGB'] + combined_output_rfc_xgb_lgb['LABEL_PRED_LGB']

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
combined_output_rfc_xgb_lgb['FINAL_LABEL'] = np.where(combined_output_rfc_xgb_lgb['COMBINED_LABEL']>=2,1,0)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
temp_columns_priority = ['DECILE_RFC','DECILE_LGB','DECILE_XGB']
combined_output_rfc_xgb_lgb['Priority'] = combined_output_rfc_xgb_lgb[temp_columns_priority].median(axis=1)

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
combined_output_rfc_xgb_lgb.head()

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
fina_op = combined_output_rfc_xgb_lgb[['NR_SBSC','Priority','FINAL_LABEL']]

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
fina_op['Month'] =1

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# Compute recipe outputs from inputs
# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe
# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
dec_fin_op_3_df = fina_op # For this sample code, simply copy input to output


# Write recipe outputs
dec_fin_op_3 = dataiku.Dataset("dec_fin_op_3")
dec_fin_op_3.write_with_schema(dec_fin_op_3_df)


################################## xgb + lift and gain chart ########################################################################################
%pylab inline

def GridSearch_table_plot(grid_clf, param_name,
                          num_results=15,
                          negative=True,
                          graph=True,
                          display_all_params=True):

    '''Display grid search results

    Arguments
    ---------

    grid_clf           the estimator resulting from a grid search
                       for example: grid_clf = GridSearchCV( ...

    param_name         a string with the name of the parameter being tested

    num_results        an integer indicating the number of results to display
                       Default: 15

    negative           boolean: should the sign of the score be reversed?
                       scoring = 'neg_log_loss', for instance
                       Default: True

    graph              boolean: should a graph be produced?
                       non-numeric parameters (True/False, None) don't graph well
                       Default: True

    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?
                       Default: True

    Usage
    -----

    GridSearch_table_plot(grid_clf, "min_samples_leaf")

                          '''
    from matplotlib      import pyplot as plt
    from IPython.display import display
    import pandas as pd

    clf = grid_clf.best_estimator_
    clf_params = grid_clf.best_params_
    if negativ
    e:
        clf_score = -grid_clf.best_score_
    else:
        clf_score = grid_clf.best_score_
    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]
    cv_results = grid_clf.cv_results_

    print("best parameters: {}".format(clf_params))
    print("best score:      {:0.5f} (+/-{:0.5f})".format(clf_score, clf_stdev))
    if display_all_params:
        import pprint
        pprint.pprint(clf.get_params())

    # pick out the best results
    # =========================
    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')

    best_row = scores_df.iloc[0, :]
    if negative:
        best_mean = -best_row['mean_test_score']
    else:
        best_mean = best_row['mean_test_score']
    best_stdev = best_row['std_test_score']
    best_param = best_row['param_' + param_name]

    # display the top 'num_results' results
    # =====================================
    display(pd.DataFrame(cv_results) \
            .sort_values(by='rank_test_score').head(num_results))

    # plot the results
    # ================
    scores_df = scores_df.sort_values(by='param_' + param_name)

    if negative:
        means = -scores_df['mean_test_score']
    else:
        means = scores_df['mean_test_score']
    stds = scores_df['std_test_score']
    params = scores_df['param_' + param_name]

    # plot
    if graph:
        plt.figure(figsize=(8, 8))
        plt.errorbar(params, means, yerr=stds)

        plt.axhline(y=best_mean + best_stdev, color='red')
        plt.axhline(y=best_mean - best_stdev, color='red')
        plt.plot(best_param, best_mean, 'or')

        plt.title(param_name + " vs Score\nBest Score {:0.5f}".format(clf_score))
        plt.xlabel(param_name)
        plt.ylabel('Score')
        plt.show()


import dataiku
from dataiku import pandasutils as pdu
import pandas as pd

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve
from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import classification_report


import xgboost as xgb
import lightgbm as lgb

import warnings
warnings.filterwarnings("ignore")

from collections import Counter
import itertools
from sklearn.preprocessing import LabelBinarizer
import boruta
import shapely
# Set all options
%matplotlib inline
plt.style.use('seaborn-notebook')
plt.rcParams["figure.figsize"] = (20, 3)
pd.options.display.float_format = '{:20,.4f}'.format
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
sns.set(context="paper", font="monospace")

# Example: load a DSS dataset as a Pandas dataframe
mydataset = dataiku.Dataset("Customer_profile_df_1")
mydataset_df = mydataset.get_dataframe()

df = mydataset_df

# older list with rfc_cols

model_cols=list([
# target feature
"INACTIVITY_LABEL",
    
# label for numbers
"NR_SBSC",
    
# usage features
"AALLGROSSREVENUEM0","AALLGROSSREVENUEM1","AALLGROSSREVENUEM2",
"AAVGINMPESAL3M",
"AAVGINSENDMONEYMPESAL3M",
"AAVGOUTMPESAL3M",   
"ACALLSALLREVENUEM0","ACALLSALLREVENUEM1","ACALLSALLREVENUEM2",
"ADATACOSTM0","ADATACOSTM1","ADATACOSTM2",
"AGSMGROSSREVENUEM0","AGSMGROSSREVENUEM1","AGSMGROSSREVENUEM2",
"AINMPESAL3M",
"AINSENDMONEYMPESAL3M",
"AMAXINMPESAL3M",
"AMAXINSENDMONEYMPESAL3M",
"AMAXOUTMPESAL3M",
"AMININMPESAL3M",
"AMININSENDMONEYMPESAL3M",
"AOTHERGROSSREVENUEM0","AOTHERGROSSREVENUEM1","AOTHERGROSSREVENUEM2",
"AOUTMPESAL3M",
"NALLDOUM0","NALLDOUM1","NALLDOUM2",
"NCALLSALLMINUTESM0","NCALLSALLMINUTESM1","NCALLSALLMINUTESM2",
"NCALLSDOUM0","NCALLSDOUM1","NCALLSDOUM2",
"NDATADOUM0","NDATADOUM1","NDATADOUM2",
"NDATAMBSM0","NDATAMBSM1","NDATAMBSM2",    
"NETUBLACKLISTM0","NETUBLACKLISTM1","NETUBLACKLISTM2",
"NGSMDOUM0","NGSMDOUM1","NGSMDOUM2",   
"NMPESADOUM0","NMPESADOUM1","NMPESADOUM2",
"NOTHERDOUM0","NOTHERDOUM1","NOTHERDOUM2",
"NSMSDOUM0","NSMSDOUM1","NSMSDOUM2"  , 

# features with 5 columns
    
"AAVGBALANCEMPESAL2M","AAVGBALANCEMPESAL3M",
"AAVGBALANCEMPESAM1","AAVGBALANCEMPESAM2","AAVGBALANCEMPESAM3",
"ABALANCEMPESAL1M","ABALANCEMPESAL2M","ABALANCEMPESAL3M",
"ABALANCEMPESAM1","ABALANCEMPESAM2","ABALANCEMPESAM3",
"ACASHINOUTMPESAL2M","ACASHINOUTMPESAL3M",
"ACASHINOUTMPESAM1","ACASHINOUTMPESAM2","ACASHINOUTMPESAM3",
"ACHARGEDMPESAL2M","ACHARGEDMPESAL3M","ACHARGEDMPESAM1","ACHARGEDMPESAM2","ACHARGEDMPESAM3",
"NDAYSLIQUIDONMPESAL1M","NDAYSLIQUIDONMPESAL2M","NDAYSLIQUIDONMPESAL3M",
"NDAYSLIQUIDONMPESAM1","NDAYSLIQUIDONMPESAM2","NDAYSLIQUIDONMPESAM3",
"NINAGENTDEPOSITMPESAL2M","NINAGENTDEPOSITMPESAL3M","NINAGENTDEPOSITMPESAM1",
"NINAGENTDEPOSITMPESAM2","NINAGENTDEPOSITMPESAM3",
"NINB2CMPESAL1M","NINB2CMPESAL2M","NINB2CMPESAL3M",
"NINB2CMPESAM1","NINB2CMPESAM2","NINB2CMPESAM3",
"NINB2CORGSMPESAL1M","NINB2CORGSMPESAL2M","NINB2CORGSMPESAL3M",
"NINB2CORGSMPESAM1","NINB2CORGSMPESAM2","NINB2CORGSMPESAM3",
"NINMPESAL1M","NINMPESAL2M","NINMPESAL3M",
"NINMPESAM1","NINMPESAM2","NINMPESAM3",
"NINSENDMONEYMPESAL1M","NINSENDMONEYMPESAL2M","NINSENDMONEYMPESAL3M",
"NINSENDMONEYMPESAM1","NINSENDMONEYMPESAM2","NINSENDMONEYMPESAM3",
"NMSHWARIDEPOSITSAVINGSL1M","NMSHWARIDEPOSITSAVINGSL2M","NMSHWARIDEPOSITSAVINGSL3M",
"NMSHWARIDEPOSITSAVINGSM1","NMSHWARIDEPOSITSAVINGSM2","NMSHWARIDEPOSITSAVINGSM3",
"NOUTAGENTWITHDRAWALMPESAL1M","NOUTAGENTWITHDRAWALMPESAL2M","NOUTAGENTWITHDRAWALMPESAL3M",
"NOUTAGENTWITHDRAWALMPESAM1","NOUTAGENTWITHDRAWALMPESAM2","NOUTAGENTWITHDRAWALMPESAM3",
"NOUTAIRTIMEMPESAL1M","NOUTAIRTIMEMPESAL2M","NOUTAIRTIMEMPESAL3M",
"NOUTAIRTIMEMPESAM1","NOUTAIRTIMEMPESAM2","NOUTAIRTIMEMPESAM3",
"NOUTLNMMPESAL1M","NOUTLNMMPESAL2M","NOUTLNMMPESAL3M",
"NOUTLNMMPESAM1","NOUTLNMMPESAM2","NOUTLNMMPESAM3",
"NOUTLNMORGSMPESAL1M","NOUTLNMORGSMPESAL2M","NOUTLNMORGSMPESAL3M",
"NOUTLNMORGSMPESAM1","NOUTLNMORGSMPESAM2","NOUTLNMORGSMPESAM3",
"NOUTPAYBILLMPESAL1M","NOUTPAYBILLMPESAL2M","NOUTPAYBILLMPESAL3M",
"NOUTPAYBILLMPESAM1","NOUTPAYBILLMPESAM2","NOUTPAYBILLMPESAM3",
"NOUTPAYBILLORGSMPESAL1M","NOUTPAYBILLORGSMPESAL2M","NOUTPAYBILLORGSMPESAL3M",
"NOUTPAYBILLORGSMPESAM1","NOUTPAYBILLORGSMPESAM2","NOUTPAYBILLORGSMPESAM3",
"NOUTSENDMONEYMPESAL1M","NOUTSENDMONEYMPESAL2M","NOUTSENDMONEYMPESAL3M",
"NOUTSENDMONEYMPESAM1","NOUTSENDMONEYMPESAM2","NOUTSENDMONEYMPESAM3",
"NOUTTRANSACMPESAL1M","NOUTTRANSACMPESAL2M","NOUTTRANSACMPESAL3M",
"NOUTTRANSACMPESAM1","NOUTTRANSACMPESAM2","NOUTTRANSACMPESAM3",
"NOUTWITHDRAWALMPESAL1M","NOUTWITHDRAWALMPESAL2M","NOUTWITHDRAWALMPESAL3M",
"NOUTWITHDRAWALMPESAM1","NOUTWITHDRAWALMPESAM2","NOUTWITHDRAWALMPESAM3",
"NUNIQUEGAMBLINGORGSMPESAL1M","NUNIQUEGAMBLINGORGSMPESAL2M","NUNIQUEGAMBLINGORGSMPESAL3M",
"NUNIQUEGAMBLINGORGSMPESAM1","NUNIQUEGAMBLINGORGSMPESAM2","NUNIQUEGAMBLINGORGSMPESAM3",
"NUTILITYMPESAL1M","NUTILITYMPESAL2M","NUTILITYMPESAL3M",
"NUTILITYMPESAM1","NUTILITYMPESAM2","NUTILITYMPESAM3"
])

df = df.loc[:,model_cols]
df.shape
df = df.drop_duplicates()

df = df.drop_duplicates()

# df = df.drop(['NR_SBSC'], axis=1)
df_model = df

X_test = df_model.drop(['INACTIVITY_LABEL'], axis=1)
y_test = df_model[['INACTIVITY_LABEL',"NR_SBSC"]]


X_not_needed, X_model, y_not_needed, y_model =train_test_split(X_test,y_test,test_size=0.1,random_state=11,stratify=y_test.INACTIVITY_LABEL)
X_not_needed_1, X_model_extra, y_not_needed_1, y_model_extra =train_test_split(X_not_needed,y_not_needed,test_size=0.2,random_state=11,stratify=y_not_needed.INACTIVITY_LABEL)

X_data_extra = pd.merge(X_model_extra, y_model_extra, how="inner", on=["NR_SBSC"])

X_data_extra = X_data_extra[X_data_extra['INACTIVITY_LABEL']==1]
X_test_extra = X_data_extra.drop(['INACTIVITY_LABEL'], axis=1)
y_test_extra = X_data_extra[['INACTIVITY_LABEL',"NR_SBSC"]]

X_model = X_model.append(X_test_extra,ignore_index=True)
y_model = y_model.append(y_test_extra,ignore_index=True)

print(X_not_needed.shape,y_not_needed.shape,X_model.shape,y_model.shape)

df = X_model

del X_not_needed
del y_not_needed
del X_data_extra
del y_test_extra
del X_test_extra

df['DAYS_ACTIVE_MORETHAN30_ALL'] = np.where((df['NALLDOUM0']+df['NALLDOUM1']+df['NALLDOUM2'])>30, 1, 0)
df['DAYS_ACTIVE_MORETHAN30_GSM'] = np.where((df['NGSMDOUM0']+df['NGSMDOUM1']+df['NGSMDOUM2'])>30, 1, 0)
df['RATE_OF_CALLS_M0'] = df['ACALLSALLREVENUEM0']/df['NCALLSALLMINUTESM0']
df['RATE_OF_CALLS_M1'] = df['ACALLSALLREVENUEM1']/df['NCALLSALLMINUTESM1']
df['RATE_OF_CALLS_M2'] = df['ACALLSALLREVENUEM2']/df['NCALLSALLMINUTESM2']


df['RATIO_DOU_ALL'] = (df['NALLDOUM1']+df['NALLDOUM2'])/(df['NALLDOUM0']+df['NALLDOUM1'])
df['RATIO_DOU_GSM'] = (df['NGSMDOUM1']+df['NGSMDOUM2'])/(df['NGSMDOUM0']+df['NGSMDOUM1'])

df["RATIO_OUTINMPESAL3M"] = df['AOUTMPESAL3M']/df['AINMPESAL3M']
df["AMAXINMPESAL3M"] = df['AMAXINMPESAL3M']/df['AMININMPESAL3M']

df['RATIO_MAXMININSENDMONEYMPESAL3M'] = df['AMININSENDMONEYMPESAL3M']/df['AMAXINSENDMONEYMPESAL3M']
df['RATIO_MAXOUTMPESAL3M']=df['AMAXOUTMPESAL3M'] / df['AOUTMPESAL3M']

df['ABALANCEMPESAM_3MONTHS_1000'] = np.where((df['ABALANCEMPESAM1']+df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3'])>1000, 1, 0)
df['ABALANCEMPESAM_3MONTHS_3000'] = np.where((df['ABALANCEMPESAM1']+df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3'])<3000, 1, 0)
df['ABALANCEMPESAM_3MONTHS_COMP1_23'] = np.where((df['ABALANCEMPESAM1']-(df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3']))<0, 1, 0)
df['ABALANCEMPESAM_3MONTHS_COMP3_21'] = np.where((df['ABALANCEMPESAM3']-(df['ABALANCEMPESAM2']+df['ABALANCEMPESAM1']))>0, 1, 0)



df['R1_ACALLSALLREVENUEM0'] = df['ACALLSALLREVENUEM0']
df['R2_ACALLSALLREVENUEM0'] = (df['ACALLSALLREVENUEM0']/(df['ACALLSALLREVENUEM0']+df['ACALLSALLREVENUEM1']))
df['R3_ACALLSALLREVENUEM0'] = (df['ACALLSALLREVENUEM0']/(df['ACALLSALLREVENUEM0']+df['ACALLSALLREVENUEM1']+df['ACALLSALLREVENUEM2']))
# df = df.drop(['ACALLSALLREVENUEM0', 'ACALLSALLREVENUEM1', 'ACALLSALLREVENUEM2'], axis=1)


df['R1_AALLGROSSREVENUEM0'] = df['AALLGROSSREVENUEM0']
df['R2_AALLGROSSREVENUEM0'] = (df['AALLGROSSREVENUEM0']/(df['AALLGROSSREVENUEM0']+df['AALLGROSSREVENUEM1']))
df['R3_AALLGROSSREVENUEM0'] = (df['AALLGROSSREVENUEM0']/(df['AALLGROSSREVENUEM0']+df['AALLGROSSREVENUEM2']+df['AALLGROSSREVENUEM2']))

# df = df.drop(["AALLGROSSREVENUEM0","AALLGROSSREVENUEM1","AALLGROSSREVENUEM2"], axis=1)

df['R1_AGSMGROSSREVENUEM0'] = df['AGSMGROSSREVENUEM0']
df['R2_AGSMGROSSREVENUEM0'] = (df['AGSMGROSSREVENUEM0']/(df['AGSMGROSSREVENUEM0']+df['AGSMGROSSREVENUEM1']))
df['R3_AGSMGROSSREVENUEM0'] = (df['AGSMGROSSREVENUEM0']/(df['AGSMGROSSREVENUEM0']+df['AGSMGROSSREVENUEM1']+df['AGSMGROSSREVENUEM2']))



# df = df.drop(["AGSMGROSSREVENUEM0","AGSMGROSSREVENUEM1","AGSMGROSSREVENUEM2"], axis=1)


df['R1_AAVGBALANCEMPESAM1'] = df['AAVGBALANCEMPESAM1']
df['R2_AAVGBALANCEMPESAM1'] = (df['AAVGBALANCEMPESAM1']/(df['AAVGBALANCEMPESAL2M']))
df['R3_AAVGBALANCEMPESAM1'] = (df['AAVGBALANCEMPESAM1']/df['AAVGBALANCEMPESAL3M'])

# df = df.drop(["AAVGBALANCEMPESAM1","AAVGBALANCEMPESAM2","AAVGBALANCEMPESAM3",'AAVGBALANCEMPESAL2M','AAVGBALANCEMPESAL3M'], axis=1)

df['R1_ACASHINOUTMPESAM1'] = df['ACASHINOUTMPESAM1']
df['R2_ACASHINOUTMPESAM1'] = (df['ACASHINOUTMPESAM1']/(df['ACASHINOUTMPESAL2M']))
df['R3_ACASHINOUTMPESAM1'] = (df['ACASHINOUTMPESAM1']/df['ACASHINOUTMPESAL3M'])
# df = df.drop(["ACASHINOUTMPESAM1","ACASHINOUTMPESAM2","ACASHINOUTMPESAM3",'ACASHINOUTMPESAL2M','ACASHINOUTMPESAL3M'], axis=1)

# charged mpesa

df['R1_ACHARGEDMPESAM1'] = df['ACHARGEDMPESAM1']
df['R2_ACHARGEDMPESAM1'] = (df['ACHARGEDMPESAM1']/(df['ACHARGEDMPESAM1']+df['ACHARGEDMPESAM2']))
df['R3_ACHARGEDMPESAM1'] = (df['ACHARGEDMPESAM1']/(df['ACHARGEDMPESAM1']+df['ACHARGEDMPESAM2']+df['ACHARGEDMPESAM3']))

# df = df.drop(["ACHARGEDMPESAM1","ACHARGEDMPESAM2","ACHARGEDMPESAM3",'ACHARGEDMPESAL3M','ACHARGEDMPESAL2M'], axis=1)

df['R1_NOUTAIRTIMEMPESAM1'] = df['NOUTAIRTIMEMPESAM1']
df['R2_NOUTAIRTIMEMPESAM1'] = (df['NOUTAIRTIMEMPESAM1']/(df['NOUTAIRTIMEMPESAL2M']))
df['R3_NOUTAIRTIMEMPESAM1'] = (df['NOUTAIRTIMEMPESAM1']/df['NOUTAIRTIMEMPESAL3M'])

# df = df.drop(["NOUTAIRTIMEMPESAM1","NOUTAIRTIMEMPESAM2","NOUTAIRTIMEMPESAM3",'NOUTAIRTIMEMPESAL1M','NOUTAIRTIMEMPESAL2M','NOUTAIRTIMEMPESAL3M'], axis=1)


df['R1_NUTILITYMPESAM1'] = df['NUTILITYMPESAM1']
df['R2_NUTILITYMPESAM1'] = (df['NUTILITYMPESAM1']/(df['NUTILITYMPESAL2M']))
df['R3_NUTILITYMPESAM1'] = (df['NUTILITYMPESAM1']/df['NUTILITYMPESAL3M'])

# df = df.drop(['NUTILITYMPESAL1M', 'NUTILITYMPESAL2M', 'NUTILITYMPESAL3M', 'NUTILITYMPESAM1', 'NUTILITYMPESAM2', 'NUTILITYMPESAM3'], axis=1)


# df['R1_NUNIQUEGAMBLINGORGSMPESAM1'] = df['NUNIQUEGAMBLINGORGSMPESAM1']
# df['R2_NUNIQUEGAMBLINGORGSMPESAM1'] = (df['NUNIQUEGAMBLINGORGSMPESAM1']/(df['NUNIQUEGAMBLINGORGSMPESAL2M']))
# df['R3_NUNIQUEGAMBLINGORGSMPESAM1'] = (df['NUNIQUEGAMBLINGORGSMPESAM1']/df['NUNIQUEGAMBLINGORGSMPESAL3M'])

df = df.drop(["NUNIQUEGAMBLINGORGSMPESAL3M",'NUNIQUEGAMBLINGORGSMPESAL3M', 'NUNIQUEGAMBLINGORGSMPESAM1', 'NUNIQUEGAMBLINGORGSMPESAM2', 'NUNIQUEGAMBLINGORGSMPESAM3'], axis=1)



df["AAVGINOUTMPESAL3M"] = df["AAVGINMPESAL3M"] - df["AAVGOUTMPESAL3M"]
df["AMAXINMINMPESAL3M"] = df['AMAXINMPESAL3M']- df['AMININMPESAL3M']


# df = df.drop(["AOUTMPESAL3M","AINMPESAL3M""AMAXINSENDMONEYMPESAL3M","AINSENDMONEYMPESAL3M","AMAXOUTMPESAL3M","AMININMPESAL3M","AMAXINMPESAL3M","AMININSENDMONEYMPESAL3M"],axis=1)


df = df.fillna(0)
df = df.replace(np.inf, 0)
df = df.replace(-np.inf, 0)

result = pd.merge(df, y_model, how="inner", on=["NR_SBSC"])

result = result.drop(['NR_SBSC'], axis=1)
df_model = result

X_test = df_model.drop(['INACTIVITY_LABEL'], axis=1)
y_test = df_model[['INACTIVITY_LABEL',]]


X_train_model, X_test_model, y_train_model, y_test_model =train_test_split(X_test,y_test,test_size=0.2,random_state=11,stratify=y_test)

print(X_train_model.shape, y_train_model.shape, X_test_model.shape, y_test_model.shape)

classifier = xgb.XGBClassifier(learning_rate=0.3,random_state=11, max_depth = 4,colsample_bytree=0.8,subsample=0.7,gamma=5)

classifier_xgb = xgb.XGBClassifier(n_estimators=500,learning_rate=0.3,random_state=11, max_depth = 4,colsample_bytree=0.8,subsample=0.7,gamma=5)
classifier_xgb.fit(X_train_model,y_train_model)

classifier_xgb

y_pred = classifier_xgb.predict(X_test_model)

print('with a lower threshold of 0.33')
threshold = 0.3
predicted_proba = classifier_xgb.predict_proba(X_test_model)

y_pred_2 = (predicted_proba [:,1] >= threshold).astype('int')
print(classification_report(y_test_model, y_pred_2))#, target_names=target_names))



# y_pred_2[0:1]
# y_test_model.head(1)
pred_proba_rfc = pd.DataFrame(predicted_proba)
y_test_rfc = y_test_model.reset_index()
y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
y_test_rfc_label.head(1)


# taking y_pred - predicted probabilities and y_hat -1,0
y_pred_df=pd.DataFrame(pred_proba_rfc[1]) ### predicted probs
y_hat_df=pd.DataFrame(y_pred_2)### 1,0

#combining y_pred_df and y_hat_df and y_test
predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],10,labels=['1','2','3','4','5','6','7','8','9','10'])
predictions['LABEL_0']=1-predictions['LABEL']

#creating Pivot table
pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
                     aggfunc={'LABEL':[np.sum],
                              'LABEL_0':[np.sum],
                              'PREDICTED_1_PROB' : [np.min,np.max]})

##### We now calculate the defaulters and non-defaulters rate per decile.


pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
default_sum = pivot_df['Defaulter_Count'].sum()
non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


#####creating the cumulative sums
pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


#####adding Base value
pivot_df['Base %'] = [10,20,30,40,50,60,70,80,90,100] ######### enogh for gain chart


#########LIFT CHART ######################
pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1]
pivot_df

feat_importances = pd.Series(classifier_xgb.feature_importances_, index=X_test_model.columns)
feat_importances.nlargest(15).plot.barh(figsize=(20,20))



# del mydataset_df_test
# del mydataset_df_test_1
# del df
del df_model

import gc
gc.collect()

# Example: load a DSS dataset as a Pandas dataframe
mydataset = dataiku.Dataset("customer_profile_df_2")
mydataset_df_test = mydataset.get_dataframe()

df = mydataset_df_test

df = mydataset_df_test
# older list with rfc_cols

model_cols=list([
# target feature
"INACTIVITY_LABEL",
    
# label for numbers
"NR_SBSC",
    
# usage features
"AALLGROSSREVENUEM0","AALLGROSSREVENUEM1","AALLGROSSREVENUEM2",..


df = df.loc[:,model_cols]
df.shape
df = df.drop_duplicates()

df.shape
# df = df.drop(['NR_SBSC'], axis=1)
df_model = df

X_test = df_model.drop(['INACTIVITY_LABEL'], axis=1)
y_test = df_model[['INACTIVITY_LABEL',"NR_SBSC"]]

df = X_test

df['DAYS_ACTIVE_MORETHAN30_ALL'] = np.where((df['NALLDOUM0']+df['NALLDOUM1']+df['NALLDOUM2'])>30, 1, 0)
df['DAYS_ACTIVE_MORETHAN30_GSM'] = np.where((df['NGSMDOUM0']+df['NGSMDOUM1']+df['NGSMDOUM2'])>30, 1, 0)
df['RATE_OF_CALLS_M0'] = df['ACALLSALLREVENUEM0']/df['NCALLSALLMINUTESM0']
df['RATE_OF_CALLS_M1'] = df['ACALLSALLREVENUEM1']/df['NCALLSALLMINUTESM1']
df['RATE_OF_CALLS_M2'] = df['ACALLSALLREVENUEM2']/df['NCALLSALLMINUTESM2']

# df['DELTA_DOU_ALL'] = (df['NALLDOUM1']+df['NALLDOUM2'])-(df['NALLDOUM0']+df['NALLDOUM1'])
# df['DELTA_DOU_GSM'] = (df['NGSMDOUM1']+df['NGSMDOUM2'])-(df['NGSMDOUM0']+df['NGSMDOUM1'])
df['RATIO_DOU_ALL'] = (df['NALLDOUM1']+df['NALLDOUM2'])/(df['NALLDOUM0']+df['NALLDOUM1'])
df['RATIO_DOU_GSM'] = (df['NGSMDOUM1']+df['NGSMDOUM2'])/(df['NGSMDOUM0']+df['NGSMDOUM1'])

df["RATIO_OUTINMPESAL3M"] = df['AOUTMPESAL3M']/df['AINMPESAL3M']
df["AMAXINMPESAL3M"] = df['AMAXINMPESAL3M']/df['AMININMPESAL3M']

df['RATIO_MAXMININSENDMONEYMPESAL3M'] = df['AMININSENDMONEYMPESAL3M']/df['AMAXINSENDMONEYMPESAL3M']
df['RATIO_MAXOUTMPESAL3M']=df['AMAXOUTMPESAL3M'] / df['AOUTMPESAL3M']

df['ABALANCEMPESAM_3MONTHS_1000'] = np.where((df['ABALANCEMPESAM1']+df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3'])>1000, 1, 0)
df['ABALANCEMPESAM_3MONTHS_3000'] = np.where((df['ABALANCEMPESAM1']+df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3'])<3000, 1, 0)
df['ABALANCEMPESAM_3MONTHS_COMP1_23'] = np.where((df['ABALANCEMPESAM1']-(df['ABALANCEMPESAM2']+df['ABALANCEMPESAM3']))<0, 1, 0)
df['ABALANCEMPESAM_3MONTHS_COMP3_21'] = np.where((df['ABALANCEMPESAM3']-(df['ABALANCEMPESAM2']+df['ABALANCEMPESAM1']))>0, 1, 0)



df['R1_ACALLSALLREVENUEM0'] = df['ACALLSALLREVENUEM0']
df['R2_ACALLSALLREVENUEM0'] = (df['ACALLSALLREVENUEM0']/(df['ACALLSALLREVENUEM0']+df['ACALLSALLREVENUEM1']))
df['R3_ACALLSALLREVENUEM0'] = (df['ACALLSALLREVENUEM0']/(df['ACALLSALLREVENUEM0']+df['ACALLSALLREVENUEM1']+df['ACALLSALLREVENUEM2']))
# df = df.drop(['ACALLSALLREVENUEM0', 'ACALLSALLREVENUEM1', 'ACALLSALLREVENUEM2'], axis=1)


df['R1_AALLGROSSREVENUEM0'] = df['AALLGROSSREVENUEM0']
df['R2_AALLGROSSREVENUEM0'] = (df['AALLGROSSREVENUEM0']/(df['AALLGROSSREVENUEM0']+df['AALLGROSSREVENUEM1']))
df['R3_AALLGROSSREVENUEM0'] = (df['AALLGROSSREVENUEM0']/(df['AALLGROSSREVENUEM0']+df['AALLGROSSREVENUEM2']+df['AALLGROSSREVENUEM2']))

# df = df.drop(["AALLGROSSREVENUEM0","AALLGROSSREVENUEM1","AALLGROSSREVENUEM2"], axis=1)

df['R1_AGSMGROSSREVENUEM0'] = df['AGSMGROSSREVENUEM0']
df['R2_AGSMGROSSREVENUEM0'] = (df['AGSMGROSSREVENUEM0']/(df['AGSMGROSSREVENUEM0']+df['AGSMGROSSREVENUEM1']))
df['R3_AGSMGROSSREVENUEM0'] = (df['AGSMGROSSREVENUEM0']/(df['AGSMGROSSREVENUEM0']+df['AGSMGROSSREVENUEM1']+df['AGSMGROSSREVENUEM2']))



# df = df.drop(["AGSMGROSSREVENUEM0","AGSMGROSSREVENUEM1","AGSMGROSSREVENUEM2"], axis=1)


df['R1_AAVGBALANCEMPESAM1'] = df['AAVGBALANCEMPESAM1']
df['R2_AAVGBALANCEMPESAM1'] = (df['AAVGBALANCEMPESAM1']/(df['AAVGBALANCEMPESAL2M']))
df['R3_AAVGBALANCEMPESAM1'] = (df['AAVGBALANCEMPESAM1']/df['AAVGBALANCEMPESAL3M'])

# df = df.drop(["AAVGBALANCEMPESAM1","AAVGBALANCEMPESAM2","AAVGBALANCEMPESAM3",'AAVGBALANCEMPESAL2M','AAVGBALANCEMPESAL3M'], axis=1)

df['R1_ACASHINOUTMPESAM1'] = df['ACASHINOUTMPESAM1']
df['R2_ACASHINOUTMPESAM1'] = (df['ACASHINOUTMPESAM1']/(df['ACASHINOUTMPESAL2M']))
df['R3_ACASHINOUTMPESAM1'] = (df['ACASHINOUTMPESAM1']/df['ACASHINOUTMPESAL3M'])
# df = df.drop(["ACASHINOUTMPESAM1","ACASHINOUTMPESAM2","ACASHINOUTMPESAM3",'ACASHINOUTMPESAL2M','ACASHINOUTMPESAL3M'], axis=1)

# charged mpesa

df['R1_ACHARGEDMPESAM1'] = df['ACHARGEDMPESAM1']
df['R2_ACHARGEDMPESAM1'] = (df['ACHARGEDMPESAM1']/(df['ACHARGEDMPESAM1']+df['ACHARGEDMPESAM2']))
df['R3_ACHARGEDMPESAM1'] = (df['ACHARGEDMPESAM1']/(df['ACHARGEDMPESAM1']+df['ACHARGEDMPESAM2']+df['ACHARGEDMPESAM3']))

# df = df.drop(["ACHARGEDMPESAM1","ACHARGEDMPESAM2","ACHARGEDMPESAM3",'ACHARGEDMPESAL3M','ACHARGEDMPESAL2M'], axis=1)

df['R1_NOUTAIRTIMEMPESAM1'] = df['NOUTAIRTIMEMPESAM1']
df['R2_NOUTAIRTIMEMPESAM1'] = (df['NOUTAIRTIMEMPESAM1']/(df['NOUTAIRTIMEMPESAL2M']))
df['R3_NOUTAIRTIMEMPESAM1'] = (df['NOUTAIRTIMEMPESAM1']/df['NOUTAIRTIMEMPESAL3M'])

# df = df.drop(["NOUTAIRTIMEMPESAM1","NOUTAIRTIMEMPESAM2","NOUTAIRTIMEMPESAM3",'NOUTAIRTIMEMPESAL1M','NOUTAIRTIMEMPESAL2M','NOUTAIRTIMEMPESAL3M'], axis=1)


df['R1_NUTILITYMPESAM1'] = df['NUTILITYMPESAM1']
df['R2_NUTILITYMPESAM1'] = (df['NUTILITYMPESAM1']/(df['NUTILITYMPESAL2M']))
df['R3_NUTILITYMPESAM1'] = (df['NUTILITYMPESAM1']/df['NUTILITYMPESAL3M'])

# df = df.drop(['NUTILITYMPESAL1M', 'NUTILITYMPESAL2M', 'NUTILITYMPESAL3M', 'NUTILITYMPESAM1', 'NUTILITYMPESAM2', 'NUTILITYMPESAM3'], axis=1)


# df['R1_NUNIQUEGAMBLINGORGSMPESAM1'] = df['NUNIQUEGAMBLINGORGSMPESAM1']
# df['R2_NUNIQUEGAMBLINGORGSMPESAM1'] = (df['NUNIQUEGAMBLINGORGSMPESAM1']/(df['NUNIQUEGAMBLINGORGSMPESAL2M']))
# df['R3_NUNIQUEGAMBLINGORGSMPESAM1'] = (df['NUNIQUEGAMBLINGORGSMPESAM1']/df['NUNIQUEGAMBLINGORGSMPESAL3M'])

df = df.drop(["NUNIQUEGAMBLINGORGSMPESAL3M",'NUNIQUEGAMBLINGORGSMPESAL3M', 'NUNIQUEGAMBLINGORGSMPESAM1', 'NUNIQUEGAMBLINGORGSMPESAM2', 'NUNIQUEGAMBLINGORGSMPESAM3'], axis=1)



df["AAVGINOUTMPESAL3M"] = df["AAVGINMPESAL3M"] - df["AAVGOUTMPESAL3M"]
df["AMAXINMINMPESAL3M"] = df['AMAXINMPESAL3M']- df['AMININMPESAL3M']


# df = df.drop(["AOUTMPESAL3M","AINMPESAL3M""AMAXINSENDMONEYMPESAL3M","AINSENDMONEYMPESAL3M","AMAXOUTMPESAL3M","AMININMPESAL3M","AMAXINMPESAL3M","AMININSENDMONEYMPESAL3M"],axis=1)


df = df.fillna(0)
df = df.replace(np.inf, 0)
X_test = df

# df = df.drop(['NR_SBSC'], axis=1)
# df_model = df

X_test = X_test.drop(['NR_SBSC'], axis=1)
y_test_1 = y_test['INACTIVITY_LABEL']


print('with a lower threshold of 0.33')
threshold = 0.4
predicted_proba = classifier_xgb.predict_proba(X_test)

y_pred_3 = (predicted_proba [:,1] >= threshold).astype('int')
print(classification_report(y_test_1, y_pred_3))#, target_names=target_names))

print(confusion_matrix(y_test_1, y_pred_3))

##### y_pred_2[0:1]
# y_test_model.head(1)
pred_proba_rfc = pd.DataFrame(predicted_proba)
y_test_rfc = y_test_1.reset_index()
y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
y_test_rfc_label.head(1)


# taking y_pred - predicted probabilities and y_hat -1,0
y_pred_df=pd.DataFrame(pred_proba_rfc[1]) ### predicted probs
y_hat_df=pd.DataFrame(y_pred_2)### 1,0

#combining y_pred_df and y_hat_df and y_test
predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],10,labels=['1','2','3','4','5','6','7','8','9','10'])
predictions['LABEL_0']=1-predictions['LABEL']

#creating Pivot table
pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
                     aggfunc={'LABEL':[np.sum],
                              'LABEL_0':[np.sum],
                              'PREDICTED_1_PROB' : [np.min,np.max]})


##### We now calculate the defaulters and non-defaulters rate per decile.


pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
default_sum = pivot_df['Defaulter_Count'].sum()
non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


#####creating the cumulative sums
pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


#####adding Base value
pivot_df['Base %'] = [10,20,30,40,50,60,70,80,90,100] ######### enogh for gain chart

#########LIFT CHART ######################
pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1]
pivot_df






param_grid = { 
    
#     'max_delta_step': [0,1,2,4],
#     'min_child_weight': [1, 5, 10],
#     'gamma': [0.5, 1, 1.5, 2, 5],
#     'subsample': [0.6, 0.8, 0.7],
#     'colsample_bytree': [0.6, 0.8, 1.0],
#     'max_depth': [4,8]
#     'learning_rate': [0.09,0.1,0.3,0.5,0.9]
   
}

param_grid

# cv_params = {'subsample': [0.4,0.5,0.6,0.7], 'max_delta_step': [0,1,2,4]}
# cv_params = {'learning_rate': [0.1,0.13,0.15,0.17,0.2,0.22,0.25,0.3]}
# cv_params = {'learning_rate': [0.01,0.02,0.05,0.09,0.08,0.1,0.11]} -0.09

classifier = xgb.XGBClassifier(learning_rate=0.3,random_state=11, max_depth = 4,colsample_bytree=0.8,subsample=0.7,gamma=5)
csv = GridSearchCV(classifier, param_grid, scoring = 'roc_auc', cv = 5, verbose=2, n_jobs=-10)
csv.fit(X_train_model, y_train_model)

csv.best_params_

GridSearch_table_plot(csv,'gamma')









rfc=RandomForestClassifier(n_estimators=500,max_depth=24,random_state=11,min_samples_split=4,criterion='gini',n_jobs=-1)
rfc.fit(X_train_model, y_train_model)
rfc.fit(X_train_model,y_train_model)
y_pred = rfc.predict(X_test_model)

feat_importances = pd.Series(rfc.feature_importances_, index=X_test_model.columns)
feat_importances.nlargest(25)

feat = list(feat_importances.nlargest(25).index)

feat

imp_cols = ['ACASHINOUTMPESAM3',
 'ABALANCEMPESAM3',
 'AAVGBALANCEMPESAM3',
 'NGSMDOUM2',

 'NALLDOUM2',
 'NOUTTRANSACMPESAM3',
 'NCALLSDOUM2',
 'AOTHERGROSSREVENUEM2',
 'AGSMGROSSREVENUEM2',
 'NDAYSLIQUIDONMPESAM3',
 'NOTHERDOUM2',

 'NOTHERDOUM1',
 'NETUBLACKLISTM2',
 'NOTHERDOUM0',
 'NCALLSALLMINUTESM2',
 
 'NGSMDOUM1',
 'AALLGROSSREVENUEM2',
 'NETUBLACKLISTM1',
 'ACASHINOUTMPESAM2',
 'INACTIVITY_LABEL']

mydataset_df[imp_cols].head()

temp_set = mydataset_df[imp_cols].fillna(0)

pwd

temp_set.shape

temp_set.to_csv("/data/dataiku/VOICEINACTIVITY/data_results.csv")

