
############################## RFC with lift charts #############################################################################
%pylab inline



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve
from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import classification_report


import xgboost as xgb
import lightgbm as lgb

import warnings
warnings.filterwarnings("ignore")

from collections import Counter
import itertools
from sklearn.preprocessing import LabelBinarizer
import boruta
import shapely
# Set all options
%matplotlib inline
plt.style.use('seaborn-notebook')
plt.rcParams["figure.figsize"] = (20, 3)
pd.options.display.float_format = '{:20,.4f}'.format
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
sns.set(context="paper", font="monospace")


my_dataset_df = mydataset_df.fillna(0)

# older list with rfc_cols

model_cols=list([
# target feature
"INACTIVITY_LABEL",
    
# label for numbers
"NR_SBSC",
 
df[df.duplicated()].shape

df = df.drop_duplicates()

# df = df.drop(['NR_SBSC'], axis=1)
df_model = df

X_test = df_model.drop(['INACTIVITY_LABEL'], axis=1)
y_test = df_model[['INACTIVITY_LABEL',"NR_SBSC"]]


X_not_needed, X_model, y_not_needed, y_model =train_test_split(X_test,y_test,test_size=0.1,random_state=11,stratify=y_test.INACTIVITY_LABEL)

X_not_needed_1, X_model_extra, y_not_needed_1, y_model_extra =train_test_split(X_not_needed,y_not_needed,test_size=0.2,random_state=11,stratify=y_not_needed.INACTIVITY_LABEL)

X_data_extra = pd.merge(X_model_extra, y_model_extra, how="inner", on=["NR_SBSC"])

X_data_extra = X_data_extra[X_data_extra['INACTIVITY_LABEL']==1]
X_test_extra = X_data_extra.drop(['INACTIVITY_LABEL'], axis=1)
y_test_extra = X_data_extra[['INACTIVITY_LABEL',"NR_SBSC"]]

X_model = X_model.append(X_test_extra,ignore_index=True)
y_model = y_model.append(y_test_extra,ignore_index=True)
df = X_model

y_model.INACTIVITY_LABEL.value_counts()/y_model.INACTIVITY_LABEL.count()

del X_not_needed
del y_not_needed


import numpy as np

df['DAYS_ACTIVE_MORETHAN30_ALL'] = np.where((df['NALLDOUM0']+df['NALLDOUM1']+df['NALLDOUM2'])>30, 1, 0)

# df = df.drop(["AOUTMPESAL3M","AINMPESAL3M""AMAXINSENDMONEYMPESAL3M","AINSENDMONEYMPESAL3M","AMAXOUTMPESAL3M","AMININMPESAL3M","AMAXINMPESAL3M","AMININSENDMONEYMPESAL3M"],axis=1)

df.shape

df = df.fillna(0)
df = df.replace(np.inf, 0)
# df = df.clip(df.quantile(0.0005), df.quantile(0.9995),axis=1)

result = pd.merge(df, y_model, how="inner", on=["NR_SBSC"])

X_test = df_model.drop(['INACTIVITY_LABEL'], axis=1)
y_test = df_model[['INACTIVITY_LABEL',]]


X_train_model, X_test_model, y_train_model, y_test_model =train_test_split(X_test,y_test,test_size=0.2,random_state=11,stratify=y_test)


from sklearn.metrics import make_scorer
from sklearn.metrics import roc_auc_score
roc_auc = make_scorer(roc_auc_score)

rfc = RandomForestClassifier(random_state=11)
def GridSearch_table_plot(grid_clf, param_name,
                          num_results=15,
                          negative=True,
                          graph=True,
                          display_all_params=True):

    '''Display grid search results

    Arguments
    ---------

    grid_clf           the estimator resulting from a grid search
                       for example: grid_clf = GridSearchCV( ...

    param_name         a string with the name of the parameter being tested

    num_results        an integer indicating the number of results to display
                       Default: 15

    negative           boolean: should the sign of the score be reversed?
                       scoring = 'neg_log_loss', for instance
                       Default: True

    graph              boolean: should a graph be produced?
                       non-numeric parameters (True/False, None) don't graph well
                       Default: True

    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?
                       Default: True

    Usage
    -----

    GridSearch_table_plot(grid_clf, "min_samples_leaf")

                          '''
    from matplotlib      import pyplot as plt
    from IPython.display import display
    import pandas as pd

    clf = grid_clf.best_estimator_
    clf_params = grid_clf.best_params_
    if negative:
        clf_score = -grid_clf.best_score_
    else:
        clf_score = grid_clf.best_score_
    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]
    cv_results = grid_clf.cv_results_

    print("best parameters: {}".format(clf_params))
    print("best score:      {:0.5f} (+/-{:0.5f})".format(clf_score, clf_stdev))
    if display_all_params:
        import pprint
        pprint.pprint(clf.get_params())

    # pick out the best results
    # =========================
    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')

    best_row = scores_df.iloc[0, :]
    if negative:
        best_mean = -best_row['mean_test_score']
    else:
        best_mean = best_row['mean_test_score']
    best_stdev = best_row['std_test_score']
    best_param = best_row['param_' + param_name]

    # display the top 'num_results' results
    # =====================================
    display(pd.DataFrame(cv_results) \
            .sort_values(by='rank_test_score').head(num_results))

    # plot the results
    # ================
    scores_df = scores_df.sort_values(by='param_' + param_name)

    if negative:
        means = -scores_df['mean_test_score']
    else:
        means = scores_df['mean_test_score']
    stds = scores_df['std_test_score']
    params = scores_df['param_' + param_name]

    # plot
    if graph:
        plt.figure(figsize=(8, 8))
        plt.errorbar(params, means, yerr=stds)

        plt.axhline(y=best_mean + best_stdev, color='red')
        plt.axhline(y=best_mean - best_stdev, color='red')
        plt.plot(best_param, best_mean, 'or')

        plt.title(param_name + " vs Score\nBest Score {:0.5f}".format(clf_score))
        plt.xlabel(param_name)
        plt.ylabel('Score')
        plt.show()

param_grid = { 
    'n_estimators': [ 500],
#     'max_features': ['auto'],
    'max_depth' : [24],
#     'criterion' :['gini', 'entropy'],
#     'class_weight' : ["balanced"],
#     'n_jobs': [-1],
#     'oob_score' : [True]
    'min_samples_split':[2,4,6]
    
}
from sklearn.metrics import make_scorer

f1_scorer = make_scorer(f1_score)
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5, n_jobs=-10,scoring = roc_auc)
CV_rfc.fit(X_train_model, y_train_model)

GridSearch_table_plot(CV_rfc,"min_samples_split")

# classifier_xgb = xgb.XGBClassifier(n_estimators=500,learning_rate=0.3,random_state=11, max_depth = 4,colsample_bytree=0.8,subsample=0.7,gamma=5)
# classifier_xgb.fit(X_train_model,y_train_model)

rfc=RandomForestClassifier(n_estimators=500,max_depth=24,random_state=11,min_samples_split=4,criterion='gini',n_jobs=-1)
rfc.fit(X_train_model, y_train_model)
rfc.fit(X_train_model,y_train_model)
y_pred = rfc.predict(X_test_model)

print(classification_report(y_test_model, y_pred))

confusion_matrix(y_test_model, y_pred)

print('with a lower threshold of 0.33')
threshold = 0.4
predicted_proba = rfc.predict_proba(X_test_model)

y_pred_2 = (predicted_proba [:,1] >= threshold).astype('int')
print(classification_report(y_test_model, y_pred_2))#, target_names=target_names))

confusion_matrix(y_test_model, y_pred_2)

# y_pred_2[0:1]
# y_test_model.head(1)
pred_proba_rfc = pd.DataFrame(predicted_proba)
y_test_rfc = y_test_model.reset_index()
y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
y_test_rfc_label.head(1)


# taking y_pred - predicted probabilities and y_hat -1,0
y_pred_df=pd.DataFrame(pred_proba_rfc[1]) ### predicted probs
y_hat_df=pd.DataFrame(y_pred_2)### 1,0

#combining y_pred_df and y_hat_df and y_test
predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],12,labels=['1','2','3','4','5','6','7','8','9','10'])
predictions['LABEL_0']=1-predictions['LABEL']

#creating Pivot table
pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
                     aggfunc={'LABEL':[np.sum],
                              'LABEL_0':[np.sum],
                              'PREDICTED_1_PROB' : [np.min,np.max]})

##### We now calculate the defaulters and non-defaulters rate per decile.


pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
default_sum = pivot_df['Defaulter_Count'].sum()
non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


#####creating the cumulative sums
pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


#####adding Base value
pivot_df['Base %'] = [10,20,30,40,50,60,70,80,90,100] ######### enogh for gain chart

#########LIFT CHART ######################
pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1]
pivot_df

feat_importances = pd.Series(rfc.feature_importances_, index=X_test_model.columns)
feat_importances.nlargest(15).plot.barh(figsize=(10,10))

import gc
gc.collect()





def GridSearch_table_plot(grid_clf, param_name,
                          num_results=15,
                          negative=True,
                          graph=True,
                          display_all_params=True):

    '''Display grid search results

    Arguments
    ---------

    grid_clf           the estimator resulting from a grid search
                       for example: grid_clf = GridSearchCV( ...

    param_name         a string with the name of the parameter being tested

    num_results        an integer indicating the number of results to display
                       Default: 15

    negative           boolean: should the sign of the score be reversed?
                       scoring = 'neg_log_loss', for instance
                       Default: True

    graph              boolean: should a graph be produced?
                       non-numeric parameters (True/False, None) don't graph well
                       Default: True

    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?
                       Default: True

    Usage
    -----

    GridSearch_table_plot(grid_clf, "min_samples_leaf")

                          '''
    from matplotlib      import pyplot as plt
    from IPython.display import display
    import pandas as pd

    clf = grid_clf.best_estimator_
    clf_params = grid_clf.best_params_
    if negative:
        clf_score = -grid_clf.best_score_
    else:
        clf_score = grid_clf.best_score_
    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]
    cv_results = grid_clf.cv_results_

    print("best parameters: {}".format(clf_params))
    print("best score:      {:0.5f} (+/-{:0.5f})".format(clf_score, clf_stdev))
    if display_all_params:
        import pprint
        pprint.pprint(clf.get_params())

    # pick out the best results
    # =========================
    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')

    best_row = scores_df.iloc[0, :]
    if negative:
        best_mean = -best_row['mean_test_score']
    else:
        best_mean = best_row['mean_test_score']
    best_stdev = best_row['std_test_score']
    best_param = best_row['param_' + param_name]

    # display the top 'num_results' results
    # =====================================
    display(pd.DataFrame(cv_results) \
            .sort_values(by='rank_test_score').head(num_results))

    # plot the results
    # ================
    scores_df = scores_df.sort_values(by='param_' + param_name)

    if negative:
        means = -scores_df['mean_test_score']
    else:
        means = scores_df['mean_test_score']
    stds = scores_df['std_test_score']
    params = scores_df['param_' + param_name]

    # plot
    if graph:
        plt.figure(figsize=(8, 8))
        plt.errorbar(params, means, yerr=stds)

        plt.axhline(y=best_mean + best_stdev, color='red')
        plt.axhline(y=best_mean - best_stdev, color='red')
        plt.plot(best_param, best_mean, 'or')

        plt.title(param_name + " vs Score\nBest Score {:0.5f}".format(clf_score))
        plt.xlabel(param_name)
        plt.ylabel('Score')
        plt.show()
