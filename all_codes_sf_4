
###################### LGB training + lift chaart ###################################################################
%pylab inline

def GridSearch_table_plot(grid_clf, param_name,
                          num_results=15,
                          negative=True,
                          graph=True,
                          display_all_params=True):

    '''Display grid search results

    Arguments
    ---------

    grid_clf           the estimator resulting from a grid search
                       for example: grid_clf = GridSearchCV( ...

    param_name         a string with the name of the parameter being tested

    num_results        an integer indicating the number of results to display
                       Default: 15

    negative           boolean: should the sign of the score be reversed?
                       scoring = 'neg_log_loss', for instance
                       Default: True

    graph              boolean: should a graph be produced?
                       non-numeric parameters (True/False, None) don't graph well
                       Default: True

    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?
                       Default: True

    Usage
    -----

    GridSearch_table_plot(grid_clf, "min_samples_leaf")

                          '''
    from matplotlib      import pyplot as plt
    from IPython.display import display
    import pandas as pd

    clf = grid_clf.best_estimator_
    clf_params = grid_clf.best_params_
    if negative:
        clf_score = -grid_clf.best_score_
    else:
        clf_score = grid_clf.best_score_
    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]
    cv_results = grid_clf.cv_results_

    print("best parameters: {}".format(clf_params))
    print("best score:      {:0.5f} (+/-{:0.5f})".format(clf_score, clf_stdev))
    if display_all_params:
        import pprint
        pprint.pprint(clf.get_params())

    # pick out the best results
    # =========================
    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')

    best_row = scores_df.iloc[0, :]
    if negative:
        best_mean = -best_row['mean_test_score']
    else:
        best_mean = best_row['mean_test_score']
    best_stdev = best_row['std_test_score']
    best_param = best_row['param_' + param_name]

    # display the top 'num_results' results
    # =====================================
    display(pd.DataFrame(cv_results) \
            .sort_values(by='rank_test_score').head(num_results))

    # plot the results
    # ================
    scores_df = scores_df.sort_values(by='param_' + param_name)

    if negative:
        means = -scores_df['mean_test_score']
    else:
        means = scores_df['mean_test_score']
    stds = scores_df['std_test_score']
    params = scores_df['param_' + param_name]

    # plot
    if graph:
        plt.figure(figsize=(8, 8))
        plt.errorbar(params, means, yerr=stds)

        plt.axhline(y=best_mean + best_stdev, color='red')
        plt.axhline(y=best_mean - best_stdev, color='red')
        plt.plot(best_param, best_mean, 'or')

        plt.title(param_name + " vs Score\nBest Score {:0.5f}".format(clf_score))
        plt.xlabel(param_name)
        plt.ylabel('Score')
        plt.show()


import dataiku
from dataiku import pandasutils as pdu
import pandas as pd

import pandas as pd

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display

from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve
from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import classification_report


import xgboost as xgb
import lightgbm as lgb

import warnings
warnings.filterwarnings("ignore")

from collections import Counter
import itertools
from sklearn.preprocessing import LabelBinarizer
import boruta
import shapely
# Set all options
%matplotlib inline
plt.style.use('seaborn-notebook')
plt.rcParams["figure.figsize"] = (20, 3)
pd.options.display.float_format = '{:20,.4f}'.format
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
sns.set(context="paper", font="monospace")

import dataiku
from dataiku import pandasutils as pdu
import pandas as pd

# Example: load a DSS dataset as a Pandas dataframe
mydataset = dataiku.Dataset("dec_1model")
mydataset_df = mydataset.get_dataframe()

mydataset_1 = dataiku.Dataset("dec_1")
mydataset_df_1 = mydataset.get_dataframe()

df=mydataset_df

df = df.drop_duplicates()

df =df.drop_duplicates(subset='NR_SBSC', keep='first')

# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
# older list with rfc_cols

model_cols=list([
# target feature
"INACTIVITY_LABEL",

# label for numbers
"NR_SBSC",
# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE
df = df.loc[:,model_cols]

df_model = df

X_test = df_model.drop(['INACTIVITY_LABEL','NR_SBSC'], axis=1)
X_msisdn = df_model['NR_SBSC']
y_test = df_model['INACTIVITY_LABEL']

df = X_test
df['DAYS_ACTIVE_MORETHAN30_ALL'] = np.where((df['NALLDOUM0']+df['NALLDOUM1']+df['NALLDOUM2'])>30, 1, 0)

# df = df.drop(["AGSMGROSSREVENUEM0","AGSMGROSSREVENUEM1","AGSMGROSSREVENUEM2"], axis=1)
df = df.replace(np.inf, 0)

def outlier_detect(df):
    for i in df.describe().columns:
        Q1=df[i].quantile(0.01)
        Q2=df[i].quantile(0.99)
        x=np.array(df[i])
        p=[]
        for j in x:
            if j < Q1:
                p.append(Q1)
            elif j > Q2:
                p.append(Q2)
            else:
                p.append(j)
        df[i]=p
    return df


df_1 = outlier_detect(df)

X_train = df_1

X_train.shape

X_msisdn.shape

y_test.shape

X_train_model, X_test_model, y_train_model, y_test_model =train_test_split(X_test,y_test,test_size=0.2,random_state=11,stratify=y_test)

print(X_train_model.shape, y_train_model.shape, X_test_model.shape, y_test_model.shape)

params = {'boosting_type': 'gbdt',
          'max_depth' : -1,
          'objective': 'binary',
          'n_estimators':100,
          'num_leaves': 31,
          'learning_rate': 0.3,#-0.3/0.1
          'max_bin': 512,
          'subsample_for_bin': 200,
          'subsample': 0.7,
          'subsample_freq': 1,
          'colsample_bytree': 0.65,
          'reg_alpha': 8,
          'reg_lambda': 1
         }

# Create parameters to search
gridParams = {
    'learning_rate': [0.001,0.01,0.05,0.1,0.3],
    'n_estimators': [100,200,400],
    'num_leaves': [6,8,12,16,31],
#     'boosting_type' : ['gbdt'],
#     'objective' : ['binary'],
#     'random_state' : [501], # Updated from 'seed'
#     'colsample_bytree' : [0.6,0.65, 0.7,0.75],
    'subsample' : [0.7,0.75]
#     'reg_alpha' : [1,1.2,2,4,6,8],
#     'reg_lambda' : [1,1.2,1.4,3,6,9,12],
    }

# Create classifier to use. Note that parameters have to be input manually
# not as a dict!
mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',
          learning_rate=0.3,
          n_estimators=100,
          objective = 'binary',
          n_jobs = -1, # Updated from 'nthread'
          silent = True,
          num_leaves = 31,
          max_depth = params['max_depth'],
          max_bin = params['max_bin'],
          subsample_for_bin = params['subsample_for_bin'],
          subsample = params['subsample'],
          subsample_freq = params['subsample_freq']
#           min_split_gain = params['min_split_gain'],
#           min_child_weight = params['min_child_weight'],
#           min_child_samples = params['min_child_samples'],
#           scale_pos_weight = params['scale_pos_weight']
                        )

import gc
gc.collect()
# gc.free()

# csv = GridSearchCV(mdl, gridParams, scoring = 'f1', cv = 5, verbose=2, n_jobs=-1)
# csv.fit(X_train_model, y_train_model)
# print(csv.best_params_)

mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',
          learning_rate=0.3,
          n_estimators=100,
          objective = 'binary',
          max_depth =-1,
          num_leaves=31,
          max_bin=512,
          subsample_for_bin=200,
          subsample=0.7,
          subsample_freq=1,
          colsample_bytree=0.65,
          reg_alpha=8,
          reg_lambda=1
            )

mdl.fit(X_train_model,y_train_model)

y_pred = mdl.predict(X_test_model)
print(classification_report(y_test_model, y_pred))

print('with a lower threshold of 0.25')
threshold = 0.2
predicted_proba = mdl.predict_proba(X_test_model)

y_pred_2 = (predicted_proba [:,1] >= threshold).astype('int')
print(classification_report(y_test_model, y_pred_2))#, target_names=target_names))
# y_pred_2[0:1]
# y_test_model.head(1)


pred_proba_rfc = pd.DataFrame(predicted_proba)
y_test_rfc = y_test_model.reset_index()
y_test_rfc_label = y_test_rfc['INACTIVITY_LABEL']
y_test_rfc_label.head(1)


# taking y_pred - predicted probabilities and y_hat -1,0
y_pred_df=pd.DataFrame(pred_proba_rfc[1]) ### predicted probs
y_hat_df=pd.DataFrame(y_pred_2)### 1,0

#combining y_pred_df and y_hat_df and y_test
predictions=pd.concat([y_test_rfc_label,y_hat_df,y_pred_df],axis=1)
predictions.columns=['LABEL','LABEL_PRED','PREDICTED_1_PROB']
predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],10,labels=['1','2','3','4','5','6','7','8','9','10'])
predictions['LABEL_0']=1-predictions['LABEL']

#creating Pivot table
pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL','LABEL_0','PREDICTED_1_PROB'],
                     aggfunc={'LABEL':[np.sum],
                              'LABEL_0':[np.sum],
                              'PREDICTED_1_PROB' : [np.min,np.max]})

##### We now calculate the defaulters and non-defaulters rate per decile.


pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
default_sum = pivot_df['Defaulter_Count'].sum()
non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


#####creating the cumulative sums
pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


#####adding Base value
pivot_df['Base %'] = [10,20,30,40,50,60,70,80,90,100] ######### enogh for gain chart

#########LIFT CHART ######################
pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1]
pivot_df


y_hat_df.head()

y_pred_df.head()



threshold = 0.2
predicted_proba = mdl.predict_proba(X_test_model)

y_pred_2 = (predicted_proba [:,1] >= threshold).astype('int')

predicted_proba_df = pd.DataFrame(predicted_proba)
y_hat = pd.DataFrame(y_pred_2)
y_pred_df=pd.DataFrame(predicted_proba_df[1])


#combining y_pred_df and y_hat_df and y_test
predictions=pd.concat([y_hat_df,y_pred_df],axis=1)
predictions.columns=['LABEL_PRED','PREDICTED_1_PROB']
predictions['PREDICTED_0_PROB']=1-predictions['PREDICTED_1_PROB']
predictions['DECILE'] = pd.qcut(predictions['PREDICTED_1_PROB'],10,labels=['1','2','3','4','5','6','7','8','9','10'])
predictions['LABEL_0']=1-predictions['LABEL_PRED']

#creating Pivot table
pivot_df= pd.pivot_table(data=predictions,index=['DECILE'],values=['LABEL_PRED','LABEL_0','PREDICTED_1_PROB'],
                     aggfunc={'LABEL_PRED':[np.sum],
                              'LABEL_0':[np.sum],
                              'PREDICTED_1_PROB' : [np.min,np.max]})

##### We now calculate the defaulters and non-defaulters rate per decile.


pivot_df.columns = ['Defaulter_Count','Non-Defaulter_Count','max_score','min_score']
pivot_df['Total_Cust'] = pivot_df['Defaulter_Count']+pivot_df['Non-Defaulter_Count']

pivot_df = pivot_df.sort_values(by='min_score',ascending=False)

pivot_df['Default_Rate'] = (pivot_df['Defaulter_Count'] / pivot_df['Total_Cust']).apply('{0:.2%}'.format)
default_sum = pivot_df['Defaulter_Count'].sum()
non_default_sum = pivot_df['Non-Defaulter_Count'].sum()
pivot_df['Default %'] = (pivot_df['Defaulter_Count']/default_sum).apply('{0:.2%}'.format)
pivot_df['Non_Default %'] = (pivot_df['Non-Defaulter_Count']/non_default_sum).apply('{0:.2%}'.format)


#####creating the cumulative sums
pivot_df['default_cum%'] = np.round(((pivot_df['Defaulter_Count'] / pivot_df['Defaulter_Count'].sum()).cumsum()), 4) * 100


#####adding Base value
pivot_df['Base %'] = [10,20,30,40,50,60,70,80,90,100] ######### enogh for gain chart

#########LIFT CHART ######################
pivot_df['lift'] = (pivot_df['default_cum%']/pivot_df['Base %'])
# final2['lift_test'] = (final['default_cum%_test']/final['Base %'])
pivot_df['Baseline']  = [1,1,1,1,1,1,1,1,1,1]
pivot_df

************************************************************************************************************************
************************************************************************************************************************
